{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Parsing dei documenti\n",
    "doc1 = nlp(\"The phases are TextPlanning , SentencePlanning and Realization\")\n",
    "doc2 = nlp(\"The phases are TextPlanning which is blue, SentencePlanning which is green, and Realization which is red.\")\n",
    "\n",
    "def simple_tree_similarity(doc1, doc2, dep_to_ignore={\"punct\", \"cc\", \"aux\", \"det\", \"prep\", \"mark\"}):\n",
    "    # Conta le corrispondenze e le discrepanze nei token e nelle dipendenze\n",
    "    matches = 0\n",
    "    total = 0\n",
    "    for token1 in doc1:\n",
    "        \n",
    "        if token1.dep_ in dep_to_ignore:\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        for token2 in doc2:\n",
    "           \n",
    "            if token1.dep_ == token2.dep_ and token1.head.lemma_ == token2.head.lemma_ and token1.lemma_ == token2.lemma_:\n",
    "                matches += 1\n",
    "            elif token1.lemma_ == token2.lemma_:\n",
    "                print(token1, token1.dep_, token1.head.lemma_, token1.lemma_)\n",
    "                print(token2, token2.dep_, token2.head.lemma_, token2.lemma_)\n",
    "\n",
    "    return matches / total if total > 0 else 0\n",
    "\n",
    "similarity = simple_tree_similarity(doc1, doc2)\n",
    "print(f\"Similarity score: {similarity:.2f}\")\n",
    "\n",
    "displacy.serve(doc1, style=\"dep\", auto_select_port=True)\n",
    "displacy.serve(doc2, style=\"dep\", auto_select_port=True )\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, id, form, lemma, pos, xpos, feats, head, dep, deps, misc):\n",
    "        self.id = id\n",
    "        self.form = form\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        self.xpos = xpos\n",
    "        self.feats = feats\n",
    "        self.head = head\n",
    "        self.dep = dep\n",
    "        self.deps = deps\n",
    "        self.misc = misc\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependencies(text):\n",
    "    baseUlr = \"http://lindat.mff.cuni.cz/services/udpipe/api/process?tokenizer&tagger&parser&model=italian-isdt-ud-2.12-230717&data=\"\n",
    "    response = requests.get(baseUlr + text)\n",
    "    data = response.json()\n",
    "    result = data[\"result\"]\n",
    "\n",
    "    text = result.split(\"\\n\")[7:]\n",
    "\n",
    "    # Converte il testo in un DataFrame\n",
    "    data = [line.split(\"\\t\") for line in text]\n",
    "    columns = [\"ID\", \"Form\", \"Lemma\", \"POS\", \"XPOS\", \"Feats\", \"Head\", \"DepRel\", \"Deps\", \"Misc\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    token_dict={}\n",
    "    tokens = []\n",
    "    for row in data:\n",
    "        if(len(row) < 10):\n",
    "            continue\n",
    "\n",
    "        head_index = int(row[6])\n",
    "        head = None\n",
    "        if head_index == 0:\n",
    "\n",
    "            head = Token(0, \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\")\n",
    "            token_dict[0] = head\n",
    "        else:\n",
    "            if head_index not in token_dict:\n",
    "                head = Token(data[head_index - 1][0], data[head_index - 1][1], data[head_index - 1][2], data[head_index - 1][3], data[head_index - 1][4], data[head_index - 1][5], data[head_index - 1][6], data[head_index - 1][7], data[head_index - 1][8], data[head_index - 1][9])\n",
    "                token_dict[head_index] = head\n",
    "            else:\n",
    "                head = token_dict[head_index]\n",
    "\n",
    "        token = None\n",
    "        if int(row[0]) in token_dict:\n",
    "            token = token_dict[int(row[0])]\n",
    "        else:\n",
    "            token = Token(row[0], row[1], row[2], row[3], row[4], row[5], head, row[7], row[8], row[9])\n",
    "            token_dict[int(row[0])] = token\n",
    "        head.add_child(token)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tree_similarity(doc1, doc2, dep_to_ignore={\"punct\", \"cc\", \"aux\", \"det\", \"prep\", \"mark\"}):\n",
    "    # Conta le corrispondenze e le discrepanze nei token e nelle dipendenze\n",
    "    matches = 0\n",
    "    total = 0\n",
    "    dict1 = {\n",
    "        \"Textplanning\": 0,\n",
    "        \"Sentenceplanning\": 0,\n",
    "        \"Realization\": 0,\n",
    "    }\n",
    "    for token1 in doc1:\n",
    "        if token1.dep in dep_to_ignore:\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        for token2 in doc2:\n",
    "           \n",
    "            if token1.dep == token2.dep and token1.head.lemma == token2.head.lemma and token1.lemma == token2.lemma:\n",
    "                matches += 1\n",
    "                # fill frame con probabilità 1\n",
    "                if(token1.lemma in dict1):\n",
    "                    dict1[token1.lemma] = 1\n",
    "                # print(token1, token1.dep, token1.head.lemma, token1.lemma, token1.pos)\n",
    "                # print(token2, token2.dep, token2.head.lemma, token2.lemma, token2.pos)\n",
    "            elif token1.lemma == token2.lemma:\n",
    "                print(token1, token1.dep, token1.head.lemma, token1.lemma, token1.pos)\n",
    "                print(token2, token2.dep, token2.head.lemma, token2.lemma, token2.pos)\n",
    "                # fill frame con probabilità minore di 1\n",
    "                if(token1.lemma in dict1):\n",
    "                    dict1[token1.lemma] = 0.5\n",
    "    return matches / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasi nsubj Textplanning fase NOUN\n",
      "fasi nsubj Sentenceplanning fase NOUN\n",
      "sono cop Textplanning essere AUX\n",
      "sono cop Sentenceplanning essere AUX\n",
      "sono cop Textplanning essere AUX\n",
      "sono cop blu essere AUX\n",
      "sono cop Textplanning essere AUX\n",
      "sono cop blu essere AUX\n",
      "sono cop Textplanning essere AUX\n",
      "sono cop blu essere AUX\n",
      "TextPlanning root ROOT Textplanning PROPN\n",
      "TextPlanning conj Sentenceplanning Textplanning PROPN\n",
      "SentencePlanning conj Textplanning Sentenceplanning PROPN\n",
      "SentencePlanning root ROOT Sentenceplanning PROPN\n",
      "Realization conj Textplanning Realization PROPN\n",
      "Realization conj Sentenceplanning Realization PROPN\n",
      "{'Textplanning': 0.5, 'Sentenceplanning': 0.5, 'Realization': 0.5}\n",
      "Similarity score: 0.00\n"
     ]
    }
   ],
   "source": [
    "doc1 = get_dependencies(\"Le fasi sono TextPlanning , SentencePlanning e Realization.\")\n",
    "doc2 = get_dependencies(\"Le fasi sono SentencePlanning, che sono blu, TextPlanning , che sono blu, e Realization che sono blu.\")\n",
    "similarity = simple_tree_similarity(doc1, doc2)\n",
    "print(f\"Similarity score: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_frame(frames, index, tokens):\n",
    "    for token in tokens:\n",
    "        for required_slot in frames[index]['required']:\n",
    "            if  token.lemma == required_slot[\"lemma\"]:\n",
    "                required_slot[\"score\"] = 1\n",
    "                for dep in required_slot[\"dependencies\"]:\n",
    "                    for child in token.children:\n",
    "                        if child.lemma == dep['to'] and child.dep == dep['type']:\n",
    "                            dep[\"score\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È una grammatica CFG in cui ogni regola di produzione ha una probabilità associata\n",
      "ogni produzione det nmod\n",
      "produzione produzione nmod nmod\n",
      "{'question': 'Cosa è una Probabilistic CFG?', 'answer': 'È una grammatica CFG in cui ogni regola di produzione ha una probabilità associata', 'required': [{'lemma': 'grammatica', 'question': [{'text': \"Cos'è una CFG?\", 'trigger': '0**'}], 'score': 1, 'weight': 0.3, 'retries': 0, 'dependencies': []}, {'lemma': 'regola', 'question': [{'text': 'Da cosa è caratterizzata una grammatica?', 'trigger': '*0*'}], 'score': 1, 'weight': 0.3, 'retries': 0, 'dependencies': [{'to': 'produzione', 'type': 'nmod', 'score': 1, 'question': [{'text': 'Che tipo di regola stiamo parlando?', 'trigger': '*1*'}], 'weight': 0.1, 'retries': 0}]}, {'lemma': 'probabilità', 'question': [{'text': 'Cosa ha associata ogni regola?', 'trigger': '*10'}], 'score': 1, 'weight': 0.3, 'retries': 0, 'dependencies': []}], 'state': '000'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "f = open('frames.json')\n",
    "frames = json.load(f)\n",
    "for i in range(0,1):\n",
    "    text = input(frames[1]['question'])\n",
    "    fill_frame(frames, 1, get_dependencies(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
